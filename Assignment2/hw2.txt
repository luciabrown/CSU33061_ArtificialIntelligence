Q1 = B [’u’, ’d’, ’l’, ’r’]
Q2 = C 25
Q3 = D The difference between the updated future rewards and the old estimate
Q4 = D The prediction of the expected future rewards is better
Q5 =
Q6 =
Q8 = B TD(s,a) = r + γmaxa′ Q(s′, a′) − Q(s, a)
Q9 = C not always the same, unless the training episodes are many enough
Q10 = D has a decreasing trend with oscillation
Q11 = B more training episodes
Q12 =   